{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25443/25443 [03:30<00:00, 120.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "file_path = \"./arxiv_dataset\"\n",
    "\n",
    "# process the docs in the arxiv_dataset directory\n",
    "# split the docs into the content and metadata, delimited by \\n\\n\\n\\n\\n\n",
    "papers = []\n",
    "metadata = []\n",
    "failed = []\n",
    "for file in tqdm(glob.glob(f\"{file_path}/*.txt\")):\n",
    "    try:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read().split(\"\\n\\n\\n\\n\\n\")\n",
    "            meta = ast.literal_eval(data[0])\n",
    "            file_id = file.split(\"\\\\\")[\n",
    "                -1\n",
    "            ]  # for non-windows users, use '/' instead of '\\\\'\n",
    "            meta[\"source\"] = f\"http://arxiv.org/abs/{file_id.replace('.txt', '')}\"\n",
    "            content = data[1]\n",
    "            papers.append(content)\n",
    "            metadata.append(meta)\n",
    "    except:\n",
    "        failed.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the documents into smaller chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character splitter (option 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recommended one for generic text, split text in this order: [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     # Since our LLM has a context size of 8000, we can set the chunk size to a rather big number\n",
    "#     chunk_size=1500,\n",
    "#     chunk_overlap=20,\n",
    "#     length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "\n",
    "# texts = text_splitter.create_documents(papers, metadatas=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token splitter (option 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400,  # max 400 tokens per chunk\n",
    "    chunk_overlap=0,\n",
    "    disallowed_special=(),\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(papers, metadatas=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the split docs\n",
    "with open(\"processed_arxiv_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embeddings model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_kwargs = {\"device\": \"cuda\"}  # change to 'cpu' if you don't have a GPU\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the documents into the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 1414155/1414155 [3:47:30<00:00, 103.60docs/s] \n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    \"cs_paper_store\",\n",
    "    embeddings_model,\n",
    "    persist_directory=\"./chromadb\",\n",
    ")\n",
    "\n",
    "\n",
    "def batch_add_documents(doc_list, batch_size):\n",
    "    total_docs = len(doc_list)\n",
    "    with tqdm(total=total_docs, desc=\"Adding documents\", unit=\"docs\") as progress_bar:\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            batch = doc_list[i : i + batch_size]\n",
    "            vector_store.add_documents(batch)\n",
    "            progress_bar.update(len(batch))\n",
    "\n",
    "\n",
    "# Specify the batch size\n",
    "batch_size = 5300\n",
    "\n",
    "# Add documents in batches and show progress using tqdm\n",
    "batch_add_documents(texts, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue with long contexts\n",
    "\n",
    "No matter the architecture of your model, there is a substantial performance degradation when you include 10+ retrieved documents. In brief: When models must access relevant information in the middle of long contexts, they tend to ignore the provided documents. See: https://arxiv.org/abs/2307.03172\n",
    "\n",
    "To avoid this issue you can re-order documents after retrieval to avoid performance degradation.\n",
    "\n",
    "Retrieved from: https://python.langchain.com/docs/modules/data_connection/document_transformers/post_retrieval/long_context_reorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line if needed\n",
    "# !wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O mistral-7b-openorca.Q4_0.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# rag_template = \"\"\"You are a helpful bot who reads texts and answers questions about them.\n",
    "# Your expertise lies in the field of academic research and your main priority is ensuring that your answers are correct.\n",
    "# You will be given relevant texts, where each text is enclosed in ``` triple back ticks, and you should derive your answer from the texts as far as possible.\n",
    "# You must to attach a citation for your answers using the texts provided.\n",
    "# If you encounter a question that you do not know how to answer, just mention that you do not know the answer.\n",
    "\n",
    "# Relevant Text: {text}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate(template=rag_template, input_variables=[\"text, \" \"question\"])\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "model_kwargs = {\"device\": \"cuda\"}  # change to 'cpu' if you don't have a GPU\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    \"cs_paper_store\",\n",
    "    embeddings_model,\n",
    "    persist_directory=\"./chromadb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code: https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/llms/gpt4all.py\n",
    "\n",
    "model_name = \"./mistral-7b-openorca.Q4_0.gguf\"\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(\n",
    "    model=model_name,\n",
    "    streaming=True,\n",
    "    max_tokens=8000,\n",
    "    n_predict=4096,\n",
    "    callbacks=callbacks,\n",
    "    verbose=True,\n",
    "    echo=True,\n",
    "    device=\"gpu\",  # switch to \"cpu\" if you don't have a GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the retriever\n",
    "\n",
    "We will be using the MultiQueryRetriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1) Discussing the latest developments in AI and machine learning, particularly focusing on significant progress made in large language models.\n",
      "2) Exploring the most current breakthroughs within artificial intelligence, specifically regarding major improvements in large-scale language processing systems.\n",
      "3) Investigating recent advancements in natural language understanding technology, with a focus on cutting-edge innovations in large language model research."
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Test the retriever\n",
    "question = \"What are the recent advancements in large language models?\"\n",
    "unique_docs = multi_query_retriever.get_relevant_documents(query=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another possible retriever: WebResearchRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.utilities import GoogleSearchAPIWrapper\n",
    "# from langchain.retrievers.web_research import WebResearchRetriever\n",
    "# import logging\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# logging.basicConfig()\n",
    "# logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "\n",
    "# search = GoogleSearchAPIWrapper()\n",
    "# web_research_retriever = WebResearchRetriever.from_llm(\n",
    "#     vectorstore=vector_store,\n",
    "#     llm=llm,\n",
    "#     search=search,\n",
    "# )\n",
    "# # Test the retriever\n",
    "# docs = web_research_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever allows the combination of BM25 and semantic search (Requires a large amount of RAM, >32gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# bm25_retriever = BM25Retriever.from_documents(texts)\n",
    "# bm25_retriever.k = 2\n",
    "\n",
    "# ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[bm25_retriever, vector_store.as_retriever()],\n",
    "#     weights=[0.5, 0.5],\n",
    "# )\n",
    "\n",
    "# # test the ensemble retriever\n",
    "# docs = ensemble_retriever.get_relevant_documents(query=question)\n",
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RetrievalQA to generate citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A support vector machine (SVM) is a machine learning algorithm based on the principle of structural risk minimisation, which is based on statistical learning theory and automatically adjusts the model structure by controlling parameters to achieve empirical and structural risk minimisation. For non-linear problems, such as disk failure prediction problems, SVM uses kernel functions to map the input data into a high-dimensional space to achieve linear separability of the high-dimensional space, thus transforming non-linear problems into linear problems."
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "query = \"What is a support vector machine?\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A support vector machine (SVM) is a machine learning algorithm based on the principle of structural risk minimisation, which is based on statistical learning theory and automatically adjusts the model structure by controlling parameters to achieve empirical and structural risk minimisation. For non-linear problems, such as disk failure prediction problems, SVM uses kernel functions to map the input data into a high-dimensional space to achieve linear separability of the high-dimensional space, thus transforming non-linear problems into linear problems.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[17] Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a library for support \\nvector machines. ACM Transactions on Intelligent Systems and \\nTechnology, \\n2:27:1- \\n27:27, \\n2011. \\nSoftware \\navailable \\nat \\nhttp://www.csie.ntu.edu.tw/ cjlin/libsvm.', metadata={'Authors': 'A. Elmaizi, E. Sarhrouni, A. Hammouch, C. Nacir', 'Published': '2022-10-26', 'Summary': 'The high dimensionality of hyperspectral images consisting of several bands\\noften imposes a big computational challenge for image processing. Therefore,\\nspectral band selection is an essential step for removing the irrelevant, noisy\\nand redundant bands. Consequently increasing the classification accuracy.\\nHowever, identification of useful bands from hundreds or even thousands of\\nrelated bands is a nontrivial task. This paper aims at identifying a small set\\nof highly discriminative bands, for improving computational speed and\\nprediction accuracy. Hence, we proposed a new strategy based on joint mutual\\ninformation to measure the statistical dependence and correlation between the\\nselected bands and evaluate the relative utility of each one to classification.\\nThe proposed filter approach is compared to an effective reproduced filters\\nbased on mutual information. Simulations results on the hyperpectral image HSI\\nAVIRIS 92AV3C using the SVM classifier have shown that the effective proposed\\nalgorithm outperforms the reproduced filters strategy performance.\\n  Keywords-Hyperspectral images, Classification, band Selection, Joint Mutual\\nInformation, dimensionality reduction ,correlation, SVM.', 'Title': 'A new band selection approach based on information theory and support vector machine for hyperspectral images reduction and classification', 'source': 'http://arxiv.org/abs/2210.14621'}),\n",
       " Document(page_content='image processing, prediction systems and pattern recognition today [16]. \\n2.2.2. SVM \\nSupport vector machine is a machine learning algorithm based on the principle of structural risk \\nminimisation, which is based on statistical learning theory and automatically adjusts the model \\nstructure by controlling parameters to achieve empirical and structural risk minimisation [17]. For \\nnon-linear problems, such as disk failure prediction problems, SVM uses kernel functions to map \\nthe input data into a high-dimensional space to achieve linear separability of the high-dimensional \\nspace, thus transforming non-linear problems into linear problems. \\n \\nFigure 2.  Support vector machine classification hyperplane  \\nAs shown in Figure.2, the left and right sides of the diagram represent different types of data, A, \\nA1, A2 are the classification surfaces of the two types of data, where A1, A2 is the edge \\nclassification surface that divides the two types of data, margin is the classification interval \\nbetween the two types, and the data points on A1, A2 is the support vector. The purpose of the \\nsupport vector machine is to find an optimal classification surface that maximizes the \\nclassification interval between the two classes [18]. \\n2.2.3. Decision Tree \\nA decision tree is a tree-based machine learning algorithm that is widely used in prediction models \\nfor split problems. It uses a tree structure to extract multiple decision rules and gradually branches \\ndown from the root node to a leaf node, resulting in a final prediction [19]. In a decision tree, each \\ninternal node represents a decision to examine feature values, every leaf node symbolizes either \\na specific category or a prediction. The data set is recursively partitioned and divided to generate', metadata={'Authors': 'Shuangshuang Yuan, Peng Wu, Yuehui Chen', 'Published': '2023-10-10', 'Summary': \"Data class imbalance is a common problem in classification problems, where\\nminority class samples are often more important and more costly to misclassify\\nin a classification task. Therefore, it is very important to solve the data\\nclass imbalance classification problem. The SMART dataset exhibits an evident\\nclass imbalance, comprising a substantial quantity of healthy samples and a\\ncomparatively limited number of defective samples. This dataset serves as a\\nreliable indicator of the disc's health status. In this paper, we obtain the\\nbest balanced disk SMART dataset for a specific classification model by mixing\\nand integrating the data synthesised by multivariate generative adversarial\\nnetworks (GAN) to balance the disk SMART dataset at the data level; and combine\\nit with genetic algorithms to obtain higher disk fault classification\\nprediction accuracy on a specific classification model.\", 'Title': 'Data-level hybrid strategy selection for disk fault prediction model based on multivariate GAN', 'source': 'http://arxiv.org/abs/2310.06537'}),\n",
       " Document(page_content='idea behind SVM is to use a linear classiﬁer that maps an\\ninput dataset into a higher dimensional vector. The aim of its\\nmapping concept is to separate between the different classes by\\nmaximizing their distance. In SVMs, a nonlinear optimization\\nproblem is formulated to solve the convex objective function,\\naiming at determining the parameters of SVMs. It is also\\nnoting that SVMs leverage kernel functions, i.e. polynomial or\\nGaussian kernels, for feature mapping. Such functions are able\\nto separate data in the input space by measuring the similarity\\nbetween two data points. In this way, the inner product of the\\ninput point can be mapped into a higher dimensional space so\\nthat data can be separated [47].\\n- K-nearest neighbors (k-NN): Another supervised learning\\ntechnique is K-NN that is a lazy learning algorithm which\\nmeans the model is computed during classiﬁcation. In prac-\\ntical applications, k-NN is used mainly for regression and\\nclassiﬁcation given an unknown data distribution. It utilizes\\nthe local neighbourhood to classify a new sample by setting\\nthe parameter K as the number of nearest neighbours. Then,\\nthe distance is computed between the test point and each\\ntraining point using the distance metrics such as Euclidean\\nor Chebyshev [48].\\n- Decision trees: Decision trees aim to build a training\\nmodel that can predict the value of target variables through\\ndecision rules. Here, a tree-based architecture is used to\\nrepresent the decision trees where each leaf node is a class\\nmodel, while the training is set as the root. From the dataset\\nsamples, the learning process ﬁnds the outcome at every leaf\\nand the best class can be portioned [49].\\n2)', metadata={'Authors': 'Dinh C. Nguyen, Peng Cheng, Ming Ding, David Lopez-Perez, Pubudu N. Pathirana, Jun Li, Aruna Seneviratne, Yonghui Li, H. Vincent Poor', 'Published': '2021-04-27', 'Summary': 'Recent years have seen rapid deployment of mobile computing and Internet of\\nThings (IoT) networks, which can be mostly attributed to the increasing\\ncommunication and sensing capabilities of wireless systems. Big data analysis,\\npervasive computing, and eventually artificial intelligence (AI) are envisaged\\nto be deployed on top of the IoT and create a new world featured by data-driven\\nAI. In this context, a novel paradigm of merging AI and wireless\\ncommunications, called Wireless AI that pushes AI frontiers to the network\\nedge, is widely regarded as a key enabler for future intelligent network\\nevolution. To this end, we present a comprehensive survey of the latest studies\\nin wireless AI from the data-driven perspective. Specifically, we first propose\\na novel Wireless AI architecture that covers five key data-driven AI themes in\\nwireless networks, including Sensing AI, Network Device AI, Access AI, User\\nDevice AI and Data-provenance AI. Then, for each data-driven AI theme, we\\npresent an overview on the use of AI approaches to solve the emerging\\ndata-related problems and show how AI can empower wireless network\\nfunctionalities. Particularly, compared to the other related survey papers, we\\nprovide an in-depth discussion on the Wireless AI applications in various\\ndata-driven domains wherein AI proves extremely useful for wireless network\\ndesign and optimization. Finally, research challenges and future visions are\\nalso discussed to spur further research in this promising area.', 'Title': 'Enabling AI in Future Wireless Networks: A Data Life Cycle Perspective', 'source': 'http://arxiv.org/abs/2003.00866'}),\n",
       " Document(page_content='2 + λ∥w∥1\\n• Linear regression with the elastic-net penalty is also known as\\nelastic-net [9]:\\nmin\\nw ∥y − Xw∥2\\n2 + λα∥w∥1 + λ(1 − α)∥w∥2\\n2\\nThe penalties can also be added in other models such as logistic regres-\\nsion, support-vector machines, artificial neural networks, etc.\\n8. Support-vector machine\\nLinear and logistic regression take into account every training sample\\nin order to find the best line, which is due to their corresponding loss\\nfunctions: the squared error is zero only if the true and predicted outputs\\nare equal, and the logistic loss is always positive. One could argue that\\nthe training samples whose outputs are “easily” well predicted are not\\nrelevant: only the training samples whose outputs are not “easily” well\\npredicted or are wrongly predicted should be taken into account. The\\nsupport vector machine (SVM) is based on this principle.\\nMachine Learning for Brain Disorders, Chapter 2\\nClassic machine learning methods\\n21\\nFigure 9:\\nSupport vector machine classifier with linearly separable\\nclasses.\\nWhen two classes are linearly separable, there exists an infi-\\nnite number of hyperplanes separating them (left). The decision function\\nof the support vector machine classifier is the hyperplane that maximizes\\nthe margin, that is the distance between the hyperplane and the closest\\npoints to the hyperplane (right). Support vectors are highlighted with a\\nblack circle surrounding them.\\n8.1\\nOriginal formulation\\nThe original support vector machine was invented in 1963 and was a\\nlinear binary classification method [10]. Figure 9 illustrates the main', metadata={'Authors': 'Johann Faouzi, Olivier Colliot', 'Published': '2023-05-24', 'Summary': 'In this chapter, we present the main classic machine learning methods. A\\nlarge part of the chapter is devoted to supervised learning techniques for\\nclassification and regression, including nearest-neighbor methods, linear and\\nlogistic regressions, support vector machines and tree-based algorithms. We\\nalso describe the problem of overfitting as well as strategies to overcome it.\\nWe finally provide a brief overview of unsupervised learning methods, namely\\nfor clustering and dimensionality reduction.', 'Title': 'Classic machine learning methods', 'source': 'http://arxiv.org/abs/2310.11470'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
